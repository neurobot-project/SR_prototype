{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.register(\n",
    "    id='NChain-v1',\n",
    "    entry_point='gym.envs.toy_text:NChainEnv',\n",
    "    kwargs={'n':10},\n",
    "    max_episode_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRAgent:\n",
    "    def __init__(self, n_state, n_action, beta=2.0, epsilon = 0.1):\n",
    "        self.n_state = n_state\n",
    "        self.n_action = n_action\n",
    "        self.sr_mat = np.zeros((n_state, n_state)).astype(\"float\")\n",
    "        self.mean_r = np.zeros(n_state).astype(\"float\")\n",
    "        self.Q = np.zeros((n_state, n_action)).astype(\"float\")\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def choose_action(self, current_s, mode=\"epsilon\"):\n",
    "        if mode == \"epsilon\":\n",
    "            if np.random.sample() < self.epsilon:\n",
    "                action = np.random.choice(env.action_space.n)\n",
    "            else:\n",
    "                action = np.argmax(self.Q[current_s])\n",
    "        else:\n",
    "            action = np.argmax(self.Q[current_s])\n",
    "        return action\n",
    "    \n",
    "    def update_Q(self, state, action):\n",
    "        self.Q[state, action] = np.dot(self.sr_mat[state, :], self.mean_r)\n",
    "    \n",
    "    def td_update(self, state, next_state, reward, lr=0.01):\n",
    "        td_error = reward + np.dot(self.sr_mat[next_state, :], self.mean_r) - np.dot(self.sr_mat[state, :], self.mean_r)\n",
    "        self.mean_r = self.mean_r +  lr * td_error * self.sr_mat[state, :]\n",
    "    \n",
    "    def update_SR_mat(self, state, next_state, lr=0.01, gamma=0.99):\n",
    "        M_target = np.eye(self.n_state)[state] + gamma * self.sr_mat[next_state,:]\n",
    "        self.sr_mat[state, :] = self.sr_mat[state, :] + lr * (M_target - self.sr_mat[state, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10\n",
    "lr = 0.01\n",
    "gamma = 0.95\n",
    "b_policy = \"epsilon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"NChain-v0\")\n",
    "# env = gym.make(\"FrozenLake8x8-v0\")\n",
    "agent = SRAgent(env.observation_space.n, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0: 2412.0\n",
      "episode 1: 2104.0\n",
      "episode 2: 1904.0\n",
      "episode 3: 2170.0\n",
      "episode 4: 1774.0\n",
      "episode 5: 1938.0\n",
      "episode 6: 2050.0\n",
      "episode 7: 1996.0\n",
      "episode 8: 1708.0\n",
      "episode 9: 2038.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    c_step = 0\n",
    "    done = False\n",
    "    c_rewards = 0.0\n",
    "    while done == False:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.td_update(state, next_state, reward, lr=lr)\n",
    "        agent.update_SR_mat(state, next_state, lr=lr, gamma=gamma)\n",
    "        agent.update_Q(next_state, action)\n",
    "        state = next_state\n",
    "        c_rewards = c_rewards + reward\n",
    "#         print(agent.mean_r)\n",
    "#         print(agent.sr_mat)\n",
    "#         print(agent.Q)\n",
    "#         print(\"\\n\\n\")\n",
    "    print(\"episode \"+str(i) + \":\", c_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 93.39207304 106.52640077 150.03506566 209.56984283 170.45421786]\n",
      "[[7.96910355 2.6504179  1.0305965  0.51099141 0.84496204]\n",
      " [6.55524514 3.13834113 1.17709475 0.58985681 0.9851063 ]\n",
      " [5.65623523 1.80924258 1.63253659 0.86597424 1.49042151]\n",
      " [5.4894196  1.75336052 0.60265535 1.2581616  2.25146995]\n",
      " [5.9670853  1.92263476 0.68856911 0.31497967 3.42689528]]\n",
      "[[1427.52879252 1456.47201133]\n",
      " [1422.3661546  1421.81070493]\n",
      " [1401.44808663 1398.9206568 ]\n",
      " [1448.17742397 1285.42885315]\n",
      " [1567.69033625 1603.59918233]]\n"
     ]
    }
   ],
   "source": [
    "print(agent.mean_r)\n",
    "print(agent.sr_mat)\n",
    "print(agent.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[0.0]]*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
